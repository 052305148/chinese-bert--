{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 指定 ZIP 文件路径和提取目标目录\n",
    "zip_path = 'your_file.zip'  # 替换为你的 ZIP 文件路径\n",
    "extract_dir = './unzipped'   # 解压目标文件夹（可选，默认当前目录）\n",
    "\n",
    "# 创建目标文件夹（如果不存在）\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# 解压文件\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)  # 解压所有内容到目标目录\n",
    "\n",
    "print(f\"解压完成！文件已保存到：{os.path.abspath(extract_dir)}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3d30cecef6f967aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install gensim -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install networkx -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install scipy -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install tqdm -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ],
   "id": "b502a3397679172f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "from scipy.sparse import lil_matrix, save_npz\n",
    "import numpy as np\n",
    "import gc  # Garbage collector\n",
    "\n",
    "def get_txt_files(folder_path):\n",
    "    \"\"\"获取指定文件夹中所有 txt 文件的路径列表\"\"\"\n",
    "    txt_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                txt_files.append(os.path.join(root, file))\n",
    "    return txt_files\n",
    "\n",
    "def read_and_segment(file_path):\n",
    "    \"\"\"读取文件内容并使用 jieba 进行中文分词\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            words = jieba.cut(content)\n",
    "            return list(words)\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "        return []\n",
    "\n",
    "def is_chinese_word(word):\n",
    "    \"\"\"判断词语是否为中文词语（不包含数字和标点）\"\"\"\n",
    "    return bool(re.match(r'^[\\u4e00-\\u9fa5]+$', word))\n",
    "\n",
    "def train_word2vec_model(word_lists, vector_size=100, window=5, min_count=3, workers=16):\n",
    "    \"\"\"训练 Word2Vec 模型\"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=word_lists,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=32,\n",
    "        sg=1  # 使用 Skip-Gram 模型，更适合捕捉词语间的语义关系\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # 指定文件夹路径和参数\n",
    "    folder_path = r'D:\\专精特新文本'\n",
    "    min_word_freq = 3  # 最小词频阈值，与 Word2Vec 的 min_count 保持一致\n",
    "    threshold = 10     # 共现次数阈值\n",
    "\n",
    "    # 检查文件夹是否存在\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"错误：文件夹 {folder_path} 不存在。\")\n",
    "        return\n",
    "\n",
    "    # 获取所有 txt 文件路径\n",
    "    txt_files = get_txt_files(folder_path)\n",
    "    print(f\"共找到 {len(txt_files)} 个 txt 文件。\")\n",
    "\n",
    "    # **第一步：计算词频并过滤词汇**\n",
    "    print(\"第一步：计算词频并过滤词汇\")\n",
    "    word_freq = Counter()\n",
    "    for i, file in enumerate(txt_files):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"词频统计：已处理 {i} 个文件\")\n",
    "        words = read_and_segment(file)\n",
    "        filtered_words = [word for word in words if is_chinese_word(word)]\n",
    "        word_freq.update(filtered_words)\n",
    "\n",
    "    # 过滤低频词\n",
    "    filtered_word_freq = {word: freq for word, freq in word_freq.items() if freq >= min_word_freq}\n",
    "    vocabulary = list(filtered_word_freq.keys())\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "    # 保存词频结果\n",
    "    with open('word_freq1.csv', 'w', encoding='utf-8') as f:\n",
    "        for word, freq in sorted(filtered_word_freq.items(), key=lambda x: x[1], reverse=True):\n",
    "            f.write(f\"{word},{freq}\\n\")\n",
    "    print(f\"词频统计完成，共 {len(vocabulary)} 个词，结果已保存至 'word_freq1.csv'。\")\n",
    "\n",
    "    # 清理内存\n",
    "    del word_freq\n",
    "    gc.collect()\n",
    "\n",
    "    # **第二步：收集分词后的文档并训练 Word2Vec 模型**\n",
    "    print(\"第二步：收集分词后的文档并训练 Word2Vec 模型\")\n",
    "    all_word_lists = []\n",
    "    for i, file in enumerate(txt_files):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"收集分词：已处理 {i} 个文件\")\n",
    "        words = read_and_segment(file)\n",
    "        filtered_words = [word for word in words if word in word_to_index]\n",
    "        if len(filtered_words) > 0:\n",
    "            all_word_lists.append(filtered_words)\n",
    "\n",
    "    # 训练 Word2Vec 模型\n",
    "    model = train_word2vec_model(all_word_lists, min_count=min_word_freq)\n",
    "    model.save(\"word2vec_model.model\")\n",
    "    print(\"Word2Vec 模型训练完成，已保存至 'word2vec_model.model'\")\n",
    "\n",
    "    # **第三步：使用 Word2Vec 模型找出相似词语**\n",
    "    print(\"第三步：找出与关键词相似的词语\")\n",
    "    target_words = [\"专业化\", \"精细化\", \"特色化\", \"新颖\"]\n",
    "    similar_words = set()\n",
    "    for word in target_words:\n",
    "        if word in model.wv:\n",
    "            similar = [w for w, _ in model.wv.most_similar(word, topn=25)]\n",
    "            similar_words.update(similar)\n",
    "            print(f\"与 '{word}' 相似的词语：{[w for w in similar]}\")\n",
    "        else:\n",
    "            print(f\"词语 '{word}' 不在模型词汇表中。\")\n",
    "    similar_words.update(target_words)  # 包含关键词本身\n",
    "    similar_words = list(similar_words)\n",
    "    print(f\"共选取 {len(similar_words)} 个词语用于构建共现矩阵和网络图。\")\n",
    "\n",
    "    # **第四步：创建基于相似词的共现矩阵**\n",
    "    print(\"第四步：创建稀疏共现矩阵\")\n",
    "    similar_word_to_index = {word: i for i, word in enumerate(similar_words)}\n",
    "    vocab_size = len(similar_words)\n",
    "    cooccurrence_matrix = lil_matrix((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for doc_words in all_word_lists:\n",
    "        filtered_doc_words = [word for word in doc_words if word in similar_word_to_index]\n",
    "        if len(filtered_doc_words) > 0:\n",
    "            doc_indices = [similar_word_to_index[w] for w in set(filtered_doc_words)]\n",
    "            for i, idx1 in enumerate(doc_indices):\n",
    "                for idx2 in doc_indices[i + 1:]:\n",
    "                    cooccurrence_matrix[idx1, idx2] += 1\n",
    "                    cooccurrence_matrix[idx2, idx1] += 1\n",
    "\n",
    "    # 保存共现矩阵\n",
    "    save_npz('cooccurrence_matrix.npz', cooccurrence_matrix.tocsr())\n",
    "    print(\"共现矩阵已保存至 'cooccurrence_matrix.npz'。\")\n",
    "\n",
    "    # **第五步：构建网络图**\n",
    "    print(\"第五步：构建网络图\")\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(similar_words)\n",
    "    cx = cooccurrence_matrix.tocoo()\n",
    "    for i, j, v in zip(cx.row, cx.col, cx.data):\n",
    "        if i < j and v >= threshold:\n",
    "            G.add_edge(similar_words[i], similar_words[j], weight=float(v))\n",
    "\n",
    "    # 保存网络图\n",
    "    nx.write_gexf(G, 'cooccurrence_network.gexf')\n",
    "    print(\"网络图已保存至 'cooccurrence_network.gexf'。\")\n",
    "\n",
    "    # 清理内存\n",
    "    del all_word_lists, cooccurrence_matrix\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "99c4475271a8506f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
